{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KRAIT GPU Executor - Updated with Compilation Support\n",
        "\n",
        "This notebook monitors GitHub for kernel files and handles both compilation and execution requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "%pip install nvidia-ml-py3 pynvml\n",
        "%pip install gitpython\n",
        "%pip install requests\n",
        "%pip install torch\n",
        "%pip install numpy\n",
        "%pip install triton\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "# Run this cell to set your GitHub token\n",
        "# Replace 'your_github_token_here' with your actual token\n",
        "import os\n",
        "os.environ['GITHUB_TOKEN'] = 'your_github_token_here'  # Replace with your actual token\n",
        "print(\"✅ Environment variable set. You can now run the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import git\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "import base64\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# GitHub API configuration\n",
        "GITHUB_OWNER = \"kcharvi\"\n",
        "GITHUB_REPO = \"KRAIT\"\n",
        "GITHUB_API_BASE = \"https://api.github.com\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up GitHub token from environment variable\n",
        "import os\n",
        "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', 'YOUR_ACTUAL_GITHUB_TOKEN_HERE')\n",
        "\n",
        "if GITHUB_TOKEN == 'YOUR_ACTUAL_GITHUB_TOKEN_HERE':\n",
        "    print(\"⚠️ WARNING: GITHUB_TOKEN environment variable not set!\")\n",
        "    print(\"Please set your GitHub token in the environment or replace the placeholder above.\")\n",
        "    print(\"You can set it by running: !export GITHUB_TOKEN='your_token_here'\")\n",
        "else:\n",
        "    print(f\"✅ GitHub token loaded from environment (first 10 chars: {GITHUB_TOKEN[:10]}...)\")\n",
        "\n",
        "# Test GitHub API connection\n",
        "def test_github_connection():\n",
        "    \"\"\"Test GitHub API connection\"\"\"\n",
        "    try:\n",
        "        url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
        "            \"Accept\": \"application/vnd.github.v3+json\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ GitHub API connection successful\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ GitHub API connection failed: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GitHub API connection error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test connection\n",
        "test_github_connection()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repository configuration\n",
        "REPO_URL = \"https://github.com/kcharvi/KRAIT.git\"  # Replace with your actual repo URL\n",
        "REPO_DIR = \"/content/krait\"\n",
        "KERNELS_DIR = f\"{REPO_DIR}/gpu-executor/kernels\"\n",
        "RESULTS_DIR = f\"{REPO_DIR}/gpu-executor/results\"\n",
        "\n",
        "# Clone or update repository\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"Cloning repository from {REPO_URL}\")\n",
        "    repo = git.Repo.clone_from(REPO_URL, REPO_DIR)\n",
        "else:\n",
        "    print(f\"Updating existing repository\")\n",
        "    repo = git.Repo(REPO_DIR)\n",
        "    repo.remotes.origin.pull()\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(KERNELS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Repository setup complete\")\n",
        "print(f\"Kernels directory: {KERNELS_DIR}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_code_type(kernel_code):\n",
        "    \"\"\"Detect if code is CUDA C++ or Triton Python\"\"\"\n",
        "    if \"@triton.jit\" in kernel_code or \"import triton\" in kernel_code:\n",
        "        return \"triton\"\n",
        "    elif \"__global__\" in kernel_code or \"#include\" in kernel_code:\n",
        "        return \"cuda\"\n",
        "    else:\n",
        "        # Default to CUDA for .cu files\n",
        "        return \"cuda\"\n",
        "\n",
        "def parse_metadata(kernel_content):\n",
        "    \"\"\"Parse metadata from kernel file\"\"\"\n",
        "    metadata = {\n",
        "        \"hardware\": \"NVIDIA T4\",\n",
        "        \"backend\": \"CUDA\",\n",
        "        \"timestamp\": int(time.time()),\n",
        "        \"type\": \"execute\"  # \"execute\" or \"compile_only\"\n",
        "    }\n",
        "    \n",
        "    lines = kernel_content.split('\\n')\n",
        "    for line in lines[:10]:  # Check first 10 lines for metadata\n",
        "        if \"// Hardware:\" in line:\n",
        "            metadata[\"hardware\"] = line.split(\":\", 1)[1].strip()\n",
        "        elif \"// Backend:\" in line:\n",
        "            metadata[\"backend\"] = line.split(\":\", 1)[1].strip()\n",
        "        elif \"// Timestamp:\" in line:\n",
        "            try:\n",
        "                metadata[\"timestamp\"] = int(line.split(\":\", 1)[1].strip())\n",
        "            except:\n",
        "                pass\n",
        "        elif \"// Type:\" in line:\n",
        "            metadata[\"type\"] = line.split(\":\", 1)[1].strip()\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "def clean_kernel_code(kernel_content):\n",
        "    \"\"\"Remove metadata comments from kernel code and fix common issues\"\"\"\n",
        "    lines = kernel_content.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    \n",
        "    skip_metadata = False\n",
        "    defines = []\n",
        "    other_lines = []\n",
        "    \n",
        "    for line in lines:\n",
        "        if line.strip() == \"// COMPILATION REQUEST\" or line.strip() == \"// EXECUTION REQUEST\":\n",
        "            skip_metadata = True\n",
        "            continue\n",
        "        elif skip_metadata and line.strip() and not line.strip().startswith(\"//\"):\n",
        "            skip_metadata = False\n",
        "        \n",
        "        if not skip_metadata:\n",
        "            # Collect #define statements\n",
        "            if line.strip().startswith(\"#define\"):\n",
        "                defines.append(line)\n",
        "            else:\n",
        "                other_lines.append(line)\n",
        "    \n",
        "    # Combine: defines first, then other code\n",
        "    result_lines = defines + other_lines\n",
        "    return '\\n'.join(result_lines).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_cuda_kernel(kernel_file_path, kernel_content):\n",
        "    \"\"\"Compile CUDA kernel and return compilation results\"\"\"\n",
        "    try:\n",
        "        print(f\"Compiling CUDA kernel: {kernel_file_path}\")\n",
        "\n",
        "        # Compile kernel\n",
        "        compile_cmd = f\"nvcc -o kernel_test {kernel_file_path} -lnvToolsExt --ptxas-options=-v\"\n",
        "        print(f\"Compilation command: {compile_cmd}\")\n",
        "\n",
        "        result = subprocess.run(compile_cmd, shell=True, capture_output=True, text=True, timeout=60)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ CUDA compilation successful\")\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"message\": \"CUDA compilation successful\",\n",
        "                \"warnings\": result.stderr if result.stderr else [],\n",
        "                \"provider\": \"colab\",\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "        else:\n",
        "            print(f\"❌ CUDA compilation failed: {result.stderr}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": f\"CUDA compilation failed: {result.stderr}\",\n",
        "                \"provider\": \"colab\",\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        error_msg = \"CUDA compilation timeout (60s)\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_msg,\n",
        "            \"provider\": \"colab\",\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_msg = f\"CUDA compilation error: {str(e)}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_msg,\n",
        "            \"provider\": \"colab\",\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "\n",
        "def compile_triton_kernel(kernel_content):\n",
        "    \"\"\"Validate Triton kernel syntax\"\"\"\n",
        "    try:\n",
        "        print(f\"Validating Triton kernel syntax\")\n",
        "\n",
        "        # Basic syntax validation\n",
        "        if \"@triton.jit\" in kernel_content and \"import triton\" in kernel_content:\n",
        "            print(\"✅ Triton syntax validation successful\")\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"message\": \"Triton syntax validation successful\",\n",
        "                \"provider\": \"colab\",\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "        else:\n",
        "            error_msg = \"Invalid Triton syntax: missing @triton.jit decorator or import triton\"\n",
        "            print(f\"❌ {error_msg}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": error_msg,\n",
        "                \"provider\": \"colab\",\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Triton validation error: {str(e)}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_msg,\n",
        "            \"provider\": \"colab\",\n",
        "            \"timestamp\": time.time()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_github_git_api_fixed(file_path, content, commit_message):\n",
        "    \"\"\"Upload file using Git API with proper branch handling\"\"\"\n",
        "    try:\n",
        "        print(f\"🔄 Uploading {file_path} using Git API...\")\n",
        "        \n",
        "        headers = {\n",
        "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
        "            \"Accept\": \"application/vnd.github.v3+json\"\n",
        "        }\n",
        "        \n",
        "        # Get the latest commit from the main branch\n",
        "        ref_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/refs/heads/main\"\n",
        "        print(\"Getting latest commit from main branch...\")\n",
        "        ref_response = requests.get(ref_url, headers=headers)\n",
        "        if ref_response.status_code != 200:\n",
        "            print(f\"❌ Failed to get branch reference: {ref_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        latest_commit_sha = ref_response.json()['object']['sha']\n",
        "        print(f\"Latest commit SHA: {latest_commit_sha}\")\n",
        "        \n",
        "        # Get the commit details\n",
        "        commit_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits/{latest_commit_sha}\"\n",
        "        print(\"Getting commit details...\")\n",
        "        commit_response = requests.get(commit_url, headers=headers)\n",
        "        if commit_response.status_code != 200:\n",
        "            print(f\"❌ Failed to get commit: {commit_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        commit_data = commit_response.json()\n",
        "        current_tree_sha = commit_data['tree']['sha']\n",
        "        print(f\"Current tree SHA: {current_tree_sha}\")\n",
        "        \n",
        "        # Create blob with content\n",
        "        content_b64 = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
        "        blob_data = {\n",
        "            \"content\": content_b64,\n",
        "            \"encoding\": \"base64\"\n",
        "        }\n",
        "        \n",
        "        blob_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/blobs\"\n",
        "        print(\"🔍 Creating blob...\")\n",
        "        blob_response = requests.post(blob_url, headers=headers, json=blob_data)\n",
        "        if blob_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create blob: {blob_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        blob_sha = blob_response.json()['sha']\n",
        "        print(f\"Blob SHA: {blob_sha}\")\n",
        "        \n",
        "        # Get current tree\n",
        "        tree_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/trees/{current_tree_sha}\"\n",
        "        print(\"🔍 Getting current tree...\")\n",
        "        tree_response = requests.get(tree_url, headers=headers)\n",
        "        if tree_response.status_code != 200:\n",
        "            print(f\"❌ Failed to get tree: {tree_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        tree_data = tree_response.json()\n",
        "        tree_items = tree_data['tree']\n",
        "        \n",
        "        # Add our new file to the tree\n",
        "        new_tree_items = []\n",
        "        file_added = False\n",
        "        \n",
        "        for item in tree_items:\n",
        "            if item['path'] == file_path:\n",
        "                # Update existing file\n",
        "                new_tree_items.append({\n",
        "                    \"path\": file_path,\n",
        "                    \"mode\": \"100644\",\n",
        "                    \"type\": \"blob\",\n",
        "                    \"sha\": blob_sha\n",
        "                })\n",
        "                file_added = True\n",
        "                print(f\"📝 Updating existing file: {file_path}\")\n",
        "            else:\n",
        "                new_tree_items.append(item)\n",
        "        \n",
        "        if not file_added:\n",
        "            # Add new file\n",
        "            new_tree_items.append({\n",
        "                \"path\": file_path,\n",
        "                \"mode\": \"100644\",\n",
        "                \"type\": \"blob\",\n",
        "                \"sha\": blob_sha\n",
        "            })\n",
        "            print(f\"Adding new file: {file_path}\")\n",
        "        \n",
        "        # Create new tree\n",
        "        new_tree_data = {\n",
        "            \"base_tree\": current_tree_sha,\n",
        "            \"tree\": new_tree_items\n",
        "        }\n",
        "        \n",
        "        new_tree_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/trees\"\n",
        "        print(\"🔍 Creating new tree...\")\n",
        "        new_tree_response = requests.post(new_tree_url, headers=headers, json=new_tree_data)\n",
        "        if new_tree_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create tree: {new_tree_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        new_tree_sha = new_tree_response.json()['sha']\n",
        "        print(f\"New tree SHA: {new_tree_sha}\")\n",
        "        \n",
        "        # Create new commit\n",
        "        new_commit_data = {\n",
        "            \"message\": commit_message,\n",
        "            \"tree\": new_tree_sha,\n",
        "            \"parents\": [latest_commit_sha]\n",
        "        }\n",
        "        \n",
        "        new_commit_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits\"\n",
        "        print(\"🔍 Creating new commit...\")\n",
        "        new_commit_response = requests.post(new_commit_url, headers=headers, json=new_commit_data)\n",
        "        if new_commit_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create commit: {new_commit_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        new_commit_sha = new_commit_response.json()['sha']\n",
        "        print(f\"New commit SHA: {new_commit_sha}\")\n",
        "        \n",
        "        # Update branch reference with force update\n",
        "        ref_data = {\n",
        "            \"sha\": new_commit_sha,\n",
        "            \"force\": True\n",
        "        }\n",
        "        \n",
        "        print(\"🔍 Updating branch reference...\")\n",
        "        ref_response = requests.patch(ref_url, headers=headers, json=ref_data)\n",
        "        if ref_response.status_code != 200:\n",
        "            print(f\"❌ Failed to update branch: {ref_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"✅ Successfully uploaded to GitHub: {file_path}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading to GitHub: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test the fixed Git API upload function\n",
        "print(\"🔧 Testing Fixed Git API upload...\")\n",
        "test_content = '{\"test\": \"fixed git api upload\", \"success\": true}'\n",
        "test_result = upload_to_github_git_api_fixed(\"gpu-executor/results/test_fixed_git_api.json\", test_content, \"Test Fixed Git API upload\")\n",
        "print(f\"Test upload result: {test_result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_github_git_api(file_path, content, commit_message):\n",
        "    \"\"\"Upload file using Git API instead of Contents API\"\"\"\n",
        "    try:\n",
        "        print(f\"🔄 Uploading {file_path} using Git API...\")\n",
        "        \n",
        "        # Get current commit\n",
        "        commit_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits/d6c27a0c97f4ba35a95b2a2d4beafcc6c1d379dd\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
        "            \"Accept\": \"application/vnd.github.v3+json\"\n",
        "        }\n",
        "        \n",
        "        print(\"Getting current commit...\")\n",
        "        response = requests.get(commit_url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"❌ Failed to get commit: {response.text}\")\n",
        "            return False\n",
        "        \n",
        "        commit_data = response.json()\n",
        "        current_tree_sha = commit_data['tree']['sha']\n",
        "        print(f\"Current tree SHA: {current_tree_sha}\")\n",
        "        \n",
        "        # Create blob with content\n",
        "        content_b64 = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
        "        blob_data = {\n",
        "            \"content\": content_b64,\n",
        "            \"encoding\": \"base64\"\n",
        "        }\n",
        "        \n",
        "        blob_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/blobs\"\n",
        "        print(\"🔍 Creating blob...\")\n",
        "        blob_response = requests.post(blob_url, headers=headers, json=blob_data)\n",
        "        if blob_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create blob: {blob_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        blob_sha = blob_response.json()['sha']\n",
        "        print(f\"Blob SHA: {blob_sha}\")\n",
        "        \n",
        "        # Get current tree\n",
        "        tree_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/trees/{current_tree_sha}\"\n",
        "        print(\"🔍 Getting current tree...\")\n",
        "        tree_response = requests.get(tree_url, headers=headers)\n",
        "        if tree_response.status_code != 200:\n",
        "            print(f\"❌ Failed to get tree: {tree_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        tree_data = tree_response.json()\n",
        "        tree_items = tree_data['tree']\n",
        "        \n",
        "        # Add our new file to the tree\n",
        "        new_tree_items = []\n",
        "        file_added = False\n",
        "        \n",
        "        for item in tree_items:\n",
        "            if item['path'] == file_path:\n",
        "                # Update existing file\n",
        "                new_tree_items.append({\n",
        "                    \"path\": file_path,\n",
        "                    \"mode\": \"100644\",\n",
        "                    \"type\": \"blob\",\n",
        "                    \"sha\": blob_sha\n",
        "                })\n",
        "                file_added = True\n",
        "                print(f\"📝 Updating existing file: {file_path}\")\n",
        "            else:\n",
        "                new_tree_items.append(item)\n",
        "        \n",
        "        if not file_added:\n",
        "            # Add new file\n",
        "            new_tree_items.append({\n",
        "                \"path\": file_path,\n",
        "                \"mode\": \"100644\",\n",
        "                \"type\": \"blob\",\n",
        "                \"sha\": blob_sha\n",
        "            })\n",
        "            print(f\"Adding new file: {file_path}\")\n",
        "        \n",
        "        # Create new tree\n",
        "        new_tree_data = {\n",
        "            \"base_tree\": current_tree_sha,\n",
        "            \"tree\": new_tree_items\n",
        "        }\n",
        "        \n",
        "        new_tree_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/trees\"\n",
        "        print(\"🔍 Creating new tree...\")\n",
        "        new_tree_response = requests.post(new_tree_url, headers=headers, json=new_tree_data)\n",
        "        if new_tree_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create tree: {new_tree_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        new_tree_sha = new_tree_response.json()['sha']\n",
        "        print(f\"New tree SHA: {new_tree_sha}\")\n",
        "        \n",
        "        # Create new commit\n",
        "        new_commit_data = {\n",
        "            \"message\": commit_message,\n",
        "            \"tree\": new_tree_sha,\n",
        "            \"parents\": [commit_data['sha']]\n",
        "        }\n",
        "        \n",
        "        new_commit_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits\"\n",
        "        print(\"🔍 Creating new commit...\")\n",
        "        new_commit_response = requests.post(new_commit_url, headers=headers, json=new_commit_data)\n",
        "        if new_commit_response.status_code != 201:\n",
        "            print(f\"❌ Failed to create commit: {new_commit_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        new_commit_sha = new_commit_response.json()['sha']\n",
        "        print(f\"New commit SHA: {new_commit_sha}\")\n",
        "        \n",
        "        # Update branch reference\n",
        "        ref_url = f\"{GITHUB_API_BASE}/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/refs/heads/main\"\n",
        "        ref_data = {\n",
        "            \"sha\": new_commit_sha\n",
        "        }\n",
        "        \n",
        "        print(\"🔍 Updating branch reference...\")\n",
        "        ref_response = requests.patch(ref_url, headers=headers, json=ref_data)\n",
        "        if ref_response.status_code != 200:\n",
        "            print(f\"❌ Failed to update branch: {ref_response.text}\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"✅ Successfully uploaded to GitHub: {file_path}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading to GitHub: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test the Git API upload function\n",
        "print(\"🔧 Testing Git API upload...\")\n",
        "test_content = '{\"test\": \"git api upload\", \"success\": true}'\n",
        "test_result = upload_to_github_git_api(\"gpu-executor/results/test_git_api.json\", test_content, \"Test Git API upload\")\n",
        "print(f\"Test upload result: {test_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_kernel_file_final(kernel_file):\n",
        "    \"\"\"Final process function with enhanced GitHub upload using Git API\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n--- Processing kernel: {kernel_file.name} ---\")\n",
        "        print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        \n",
        "        # Read kernel content\n",
        "        with open(kernel_file, 'r') as f:\n",
        "            kernel_content = f.read()\n",
        "        \n",
        "        # Parse metadata\n",
        "        metadata = parse_metadata(kernel_content)\n",
        "        print(f\"Metadata: {metadata}\")\n",
        "        \n",
        "        # Clean kernel code\n",
        "        clean_code = clean_kernel_code(kernel_content)\n",
        "        \n",
        "        # Detect code type\n",
        "        code_type = detect_code_type(clean_code)\n",
        "        print(f\"Detected code type: {code_type}\")\n",
        "        \n",
        "        # Write cleaned code to file for compilation\n",
        "        with open(kernel_file, 'w') as f:\n",
        "            f.write(clean_code)\n",
        "        \n",
        "        # Process based on request type\n",
        "        if metadata[\"type\"] == \"compile_only\":\n",
        "            print(\"Processing compilation request...\")\n",
        "            if code_type == \"cuda\":\n",
        "                result = compile_cuda_kernel(str(kernel_file), clean_code)\n",
        "            else:\n",
        "                result = compile_triton_kernel(clean_code)\n",
        "        else:\n",
        "            print(\"Processing execution request...\")\n",
        "            # For execution, we'll just do compilation for now\n",
        "            if code_type == \"cuda\":\n",
        "                result = compile_cuda_kernel(str(kernel_file), clean_code)\n",
        "            else:\n",
        "                result = compile_triton_kernel(clean_code)\n",
        "        \n",
        "        # Add corrected code to result\n",
        "        if result.get(\"success\", False):\n",
        "            result[\"corrected_code\"] = clean_code\n",
        "        \n",
        "        # Determine result filename based on request type\n",
        "        if metadata[\"type\"] == \"compile_only\":\n",
        "            result_file = f\"{RESULTS_DIR}/compile_{metadata['timestamp']}_result.json\"\n",
        "        else:\n",
        "            result_file = f\"{RESULTS_DIR}/kernel_{metadata['timestamp']}_result.json\"\n",
        "        \n",
        "        # Save result locally\n",
        "        with open(result_file, 'w') as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "        \n",
        "        print(f\"Result saved locally to: {result_file}\")\n",
        "        print(f\"Result: {json.dumps(result, indent=2)}\")\n",
        "        \n",
        "        # Upload result to GitHub using Git API function\n",
        "        result_path = f\"gpu-executor/results/{os.path.basename(result_file)}\"\n",
        "        with open(result_file, 'r') as f:\n",
        "            result_content = f.read()\n",
        "        \n",
        "        upload_success = upload_to_github_git_api_fixed(result_path, result_content, f\"Result {metadata['timestamp']}\")\n",
        "        \n",
        "        # If compilation was successful, also save the corrected kernel code to GitHub\n",
        "        if result.get(\"success\", False):\n",
        "            corrected_kernel_path = f\"gpu-executor/kernels/corrected_{metadata['timestamp']}.cu\"\n",
        "            upload_success = upload_to_github_git_api_fixed(corrected_kernel_path, clean_code, f\"Corrected kernel {metadata['timestamp']}\") and upload_success\n",
        "        \n",
        "        if upload_success:\n",
        "            print(f\"✅ All uploads successful\")\n",
        "        else:\n",
        "            print(f\"⚠️ Some uploads failed, but processing complete\")\n",
        "        \n",
        "        # Wait a bit to ensure backend can fetch the result\n",
        "        time.sleep(5)\n",
        "        \n",
        "        # Remove processed kernel file locally\n",
        "        kernel_file.unlink()\n",
        "        print(f\"Kernel file removed locally: {kernel_file.name}\")\n",
        "        \n",
        "        # Note: Backend handles GitHub cleanup automatically\n",
        "        print(f\"ℹ️ Backend will handle GitHub cleanup automatically\")\n",
        "        \n",
        "        print(f\"--- Processing complete ---\\n\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing {kernel_file.name}: {str(e)}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        \n",
        "        # Save error result\n",
        "        try:\n",
        "            metadata = parse_metadata(kernel_content) if 'kernel_content' in locals() else {\"timestamp\": int(time.time())}\n",
        "            error_result = {\n",
        "                \"success\": False,\n",
        "                \"error\": error_msg,\n",
        "                \"provider\": \"colab\",\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            \n",
        "            result_file = f\"{RESULTS_DIR}/kernel_{metadata['timestamp']}_result.json\"\n",
        "            with open(result_file, 'w') as f:\n",
        "                json.dump(error_result, f, indent=2)\n",
        "            \n",
        "            print(f\"Error result saved to: {result_file}\")\n",
        "        except:\n",
        "            print(\"Failed to save error result\")\n",
        "        \n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monitor_kernels_final():\n",
        "    \"\"\"Final monitoring function with enhanced GitHub upload using Git API\"\"\"\n",
        "    print(f\"🚀 Starting KRAIT GPU Executor - Final Version with Git API\")\n",
        "    print(f\"📁 Monitoring for both compilation and execution requests\")\n",
        "    print(f\"⚡ Ready to process kernels...\")\n",
        "    print(f\"Watching directory: {KERNELS_DIR}\")\n",
        "    \n",
        "    processed_files = set()\n",
        "    git_error_count = 0\n",
        "    max_git_errors = 5\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # Pull latest changes from GitHub\n",
        "            try:\n",
        "                repo.remotes.origin.pull()\n",
        "                git_error_count = 0  # Reset error count on success\n",
        "            except Exception as e:\n",
        "                git_error_count += 1\n",
        "                # Handle broken pipe and other Git errors gracefully\n",
        "                if \"Broken pipe\" in str(e) or \"Errno 32\" in str(e):\n",
        "                    print(f\"Git connection issue ({git_error_count}/{max_git_errors}): {e}\")\n",
        "                    # Try to reinitialize the connection\n",
        "                    try:\n",
        "                        repo.remotes.origin.fetch()\n",
        "                    except:\n",
        "                        pass\n",
        "                else:\n",
        "                    print(f\"Warning: Failed to pull from GitHub ({git_error_count}/{max_git_errors}): {e}\")\n",
        "                \n",
        "                # If too many Git errors, skip this cycle\n",
        "                if git_error_count >= max_git_errors:\n",
        "                    print(\"Too many Git errors, skipping this cycle...\")\n",
        "                    time.sleep(60)  # Wait longer before retrying\n",
        "                    git_error_count = 0\n",
        "            \n",
        "            # Note: Backend handles cleanup automatically\n",
        "            \n",
        "            # Check for new kernel files\n",
        "            kernel_files = list(Path(KERNELS_DIR).glob(\"*.cu\"))\n",
        "            \n",
        "            for kernel_file in kernel_files:\n",
        "                if kernel_file.name not in processed_files:\n",
        "                    success = process_kernel_file_final(kernel_file)\n",
        "                    if success:\n",
        "                        processed_files.add(kernel_file.name)\n",
        "            \n",
        "            if not kernel_files:\n",
        "                print(f\".\", end=\"\", flush=True)  # Show activity\n",
        "            \n",
        "            time.sleep(30)  # Check every 30 seconds to reduce Git load\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nMonitoring stopped by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in monitoring loop: {e}\")\n",
        "            time.sleep(30)  # Wait longer on error\n",
        "\n",
        "\n",
        "# Start final monitoring\n",
        "print(\"\\n🚀 Starting Final Monitoring with Git API Upload...\")\n",
        "monitor_kernels_final()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
